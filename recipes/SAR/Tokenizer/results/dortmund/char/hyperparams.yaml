# Generated 2022-12-05 from:
# /netscratch/sagar/thesis/speechbrain/recipes/SAR/Tokenizer/hparams/1K_unigram_subword_bpe.yaml
# yamllint disable
# ############################################################################
# Tokenizer: subword BPE with char 1K
# Training: SAR domain dataset 100 mins
# Authors:  Abdel Heba 2021, Sangeet Sagar 2022
# ############################################################################

token_type: char       # ["unigram", "bpe", "char"]
dataset: dortmund
output_folder: results/dortmund/char
train_log: results/dortmund/char/train_log.txt

# Data files
data_folder: ../dataset/dortmund_dataset
csv_dir: ../csv_files/dortmund
train_tsv_file: ../dataset/dortmund_dataset/train.tsv
dev_tsv_file: ../dataset/dortmund_dataset/dev.tsv
test_tsv_file: ../dataset/dortmund_dataset/test.tsv
accented_letters: true
language: de
skip_prep: false

train_csv: ../csv_files/dortmund/train.csv
valid_csv: ../csv_files/dortmund/dev.csv

# Training parameters
token_output: 32  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
csv_read: wrd
bos_id: -1
eos_id: -1

tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
  model_dir: results/dortmund/char
  vocab_size: 32
  annotation_train: ../csv_files/dortmund/train.csv
  annotation_read: wrd
  model_type: char               # ["unigram", "bpe", "char"]
  character_coverage: 1.0
  annotation_list_to_check: [../csv_files/dortmund/train.csv, ../csv_files/dortmund/dev.csv]
  bos_id: -1
  eos_id: -1
