# ############################################################################
# Tokenizer: subword BPE with unigram 1K
# Training: Feuer und Flammer 17 hrs
# Authors:  Abdel Heba 2021
# ############################################################################

token_type: !ref char  # ["unigram", "bpe", "char"]
type: radio
output_folder: !ref results/<type>/<token_type>
train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: !ref /netscratch/sagar/data/Feuer-Flamme/<type>
csv_dir: !ref ../<type>
train_tsv_file: !ref <data_folder>/train.tsv
dev_tsv_file: !ref <data_folder>/dev.tsv
test_tsv_file: !ref <data_folder>/test.tsv
accented_letters: True
language: de
skip_prep: False

# train_splits: ["train-clean-100", "train-clean-360", "train-other-500"]
# dev_splits: ["dev-clean"]
# test_splits: ["test-clean", "test-other"]
train_csv: !ref <csv_dir>/train.csv
valid_csv: !ref <csv_dir>/dev.csv

# Training parameters
token_output: 32  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
csv_read: wrd
bos_id: -1
eos_id: -1

tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <train_csv>
   annotation_read: !ref <csv_read>
   model_type: !ref <token_type> # ["unigram", "bpe", "char"]
   character_coverage: !ref <character_coverage>
   annotation_list_to_check: [!ref <train_csv>, !ref <valid_csv>]
   bos_id: !ref <bos_id>
   eos_id: !ref <eos_id>
